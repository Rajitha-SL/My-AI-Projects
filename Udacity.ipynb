{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajitha-SL/My-AI-Projects/blob/AI-and-ML-learning/Udacity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an example for a typical Convolutional Block"
      ],
      "metadata": {
        "id": "BelfTejO_-mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# self.conv1 = nn.Conv2d(3 ,16, 3, padding=1)\n",
        "# self.pool = nn.MaxPool2d(2,2)\n",
        "# self.relu1 = nn.ReLU()\n",
        "# self.drop1 = nn.Dropout2d(0.2)"
      ],
      "metadata": {
        "id": "EHv_Tt-f_WPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how we do the same with nn.Sequential"
      ],
      "metadata": {
        "id": "WqCwjyHohVpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# self.conv_block = nn.Sequential(\n",
        "#    nn.Conv2d(3, 16, 3, padding = 1),\n",
        "#    nn.MaxPool(2,2),\n",
        "#    nn.ReLU(),\n",
        "#    nn.Dropout(0.2)\n",
        "# )"
      ],
      "metadata": {
        "id": "t6-RFp_ihkcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our first CNN in PyTorch"
      ],
      "metadata": {
        "id": "l4w00sJ0iU1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyCNN(nn.Module):\n",
        "  def __init__ (self, n_classes):\n",
        "    super.__init__()\n",
        "\n",
        "    # Creating layers. In this case just a standard MLP (Multi Layer Perceptron)\n",
        "    self.model = nn.Sequential(\n",
        "        # First conv + maxpool + relu\n",
        "        nn.Conv2d(3, 16, 3, padding = 1),\n",
        "        nn.MaxPool(2,2),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        # Second conv + maxpool + relu\n",
        "        nn.Conv2d(16, 32, 3, padding = 1),\n",
        "        nn.MaxPool(2,2),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        # Third conv + maxpool + relu\n",
        "        nn.Conv2d(32, 64, 3, padding = 1),\n",
        "        nn.MaxPool(2,2),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        # Flatten the 64 feature maps\n",
        "        nn.Flatten(),\n",
        "\n",
        "        # Fully connected layers. This assumes\n",
        "        # that the input image was 32X32\n",
        "        nn.Linear(1024, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(128, n_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # nn.Sequential will call the layers\n",
        "    # in the order they have been listed out\n",
        "    return self.model(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "zIFwHyToiaoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "!pip install livelossplot\n",
        "from livelossplot import PlotLosses\n",
        "from livelossplot.outputs import MatplotlibPlot\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def get_train_val_data_loaders(batch_size, valid_size, transforms, num_workers):\n",
        "\n",
        "    # Get the CIFAR10 training dataset from torchvision.datasets and set the transforms\n",
        "    # We will split this further into train and validation in this function\n",
        "    train_data = datasets.CIFAR10(\"data\", train=True, download=True, transform=transforms)\n",
        "\n",
        "    # Compute how many items we will reserve for the validation set\n",
        "    n_tot = len(train_data)\n",
        "    split = int(np.floor(valid_size * n_tot))\n",
        "\n",
        "    # compute the indices for the training set and for the validation set\n",
        "    shuffled_indices = torch.randperm(n_tot)\n",
        "    train_idx, valid_idx = shuffled_indices[split:], shuffled_indices[:split]\n",
        "\n",
        "    # define samplers for obtaining training and validation batches\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    # prepare data loaders (combine dataset and sampler)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers\n",
        "    )\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return train_loader, valid_loader\n",
        "\n",
        "\n",
        "def get_test_data_loader(batch_size, transforms, num_workers):\n",
        "    # We use the entire test dataset in the test dataloader\n",
        "    test_data = datasets.CIFAR10(\"data\", train=False, download=True, transform=transforms)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=batch_size, num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return test_loader\n",
        "\n",
        "\n",
        "def train_one_epoch(train_dataloader, model, optimizer, loss):\n",
        "    \"\"\"\n",
        "    Performs one epoch of training\n",
        "    \"\"\"\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()  # -\n",
        "\n",
        "    # Set the model in training mode\n",
        "    # (so all layers that behave differently between training and evaluation,\n",
        "    # like batchnorm and dropout, will select their training behavior)\n",
        "    model.train()  # -\n",
        "\n",
        "    # Loop over the training data\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch_idx, (data, target) in tqdm(\n",
        "        enumerate(train_dataloader),\n",
        "        desc=\"Training\",\n",
        "        total=len(train_dataloader),\n",
        "        leave=True,\n",
        "        ncols=80,\n",
        "    ):\n",
        "        # move data to GPU if available\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        # 1. clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()  # -\n",
        "        # 2. forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)  # =\n",
        "        # 3. calculate the loss\n",
        "        loss_value = loss(output, target)  # =\n",
        "        # 4. backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss_value.backward()  # -\n",
        "        # 5. perform a single optimization step (parameter update)\n",
        "        optimizer.step()  # -\n",
        "\n",
        "        # update average training loss\n",
        "        train_loss = train_loss + (\n",
        "            (1 / (batch_idx + 1)) * (loss_value.data.item() - train_loss)\n",
        "        )\n",
        "\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "def valid_one_epoch(valid_dataloader, model, loss):\n",
        "    \"\"\"\n",
        "    Validate at the end of one epoch\n",
        "    \"\"\"\n",
        "\n",
        "    # During validation we don't need to accumulate gradients\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # set the model to evaluation mode\n",
        "        # (so all layers that behave differently between training and evaluation,\n",
        "        # like batchnorm and dropout, will select their evaluation behavior)\n",
        "        model.eval()  # -\n",
        "\n",
        "        # If the GPU is available, move the model to the GPU\n",
        "        if torch.cuda.is_available():\n",
        "            model.cuda()\n",
        "\n",
        "        # Loop over the validation dataset and accumulate the loss\n",
        "        valid_loss = 0.0\n",
        "        for batch_idx, (data, target) in tqdm(\n",
        "            enumerate(valid_dataloader),\n",
        "            desc=\"Validating\",\n",
        "            total=len(valid_dataloader),\n",
        "            leave=True,\n",
        "            ncols=80,\n",
        "        ):\n",
        "            # move data to GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            # 1. forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)  # =\n",
        "            # 2. calculate the loss\n",
        "            loss_value = loss(output, target)  # =\n",
        "\n",
        "            # Calculate average validation loss\n",
        "            valid_loss = valid_loss + (\n",
        "                (1 / (batch_idx + 1)) * (loss_value.data.item() - valid_loss)\n",
        "            )\n",
        "\n",
        "    return valid_loss\n",
        "\n",
        "\n",
        "def optimize(data_loaders, model, optimizer, loss, n_epochs, save_path, interactive_tracking=False):\n",
        "    # initialize tracker for minimum validation loss\n",
        "    if interactive_tracking:\n",
        "        liveloss = PlotLosses()\n",
        "    else:\n",
        "        liveloss = None\n",
        "\n",
        "    # Loop over the epochs and keep track of the minimum of the validation loss\n",
        "    valid_loss_min = None\n",
        "    logs = {}\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        train_loss = train_one_epoch(\n",
        "            data_loaders[\"train\"], model, optimizer, loss\n",
        "        )\n",
        "\n",
        "        valid_loss = valid_one_epoch(data_loaders[\"valid\"], model, loss)\n",
        "\n",
        "        # print training/validation statistics\n",
        "        print(\n",
        "            \"Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}\".format(\n",
        "                epoch, train_loss, valid_loss\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # If the validation loss decreases by more than 1%, save the model\n",
        "        if valid_loss_min is None or (\n",
        "                (valid_loss_min - valid_loss) / valid_loss_min > 0.01\n",
        "        ):\n",
        "            print(f\"New minimum validation loss: {valid_loss:.6f}. Saving model ...\")\n",
        "\n",
        "            # Save the weights to save_path\n",
        "            torch.save(model.state_dict(), save_path)  # -\n",
        "\n",
        "            valid_loss_min = valid_loss\n",
        "\n",
        "        # Log the losses and the current learning rate\n",
        "        if interactive_tracking:\n",
        "            logs[\"loss\"] = train_loss\n",
        "            logs[\"val_loss\"] = valid_loss\n",
        "\n",
        "            liveloss.update(logs)\n",
        "            liveloss.send()\n",
        "\n",
        "\n",
        "def one_epoch_test(test_dataloader, model, loss):\n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "\n",
        "    # we do not need the gradients\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # set the model to evaluation mode\n",
        "        model.eval()  # -\n",
        "\n",
        "        # if the GPU is available, move the model to the GPU\n",
        "        if torch.cuda.is_available():\n",
        "            model = model.cuda()\n",
        "\n",
        "        # Loop over test dataset\n",
        "        # We also accumulate predictions and targets so we can return them\n",
        "        preds = []\n",
        "        actuals = []\n",
        "\n",
        "        for batch_idx, (data, target) in tqdm(\n",
        "                enumerate(test_dataloader),\n",
        "                desc='Testing',\n",
        "                total=len(test_dataloader),\n",
        "                leave=True,\n",
        "                ncols=80\n",
        "        ):\n",
        "            # move data to GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            # 1. forward pass: compute predicted outputs by passing inputs to the model\n",
        "            logits = model(data)  # =\n",
        "            # 2. calculate the loss\n",
        "            loss_value = loss(logits, target).detach()  # =\n",
        "\n",
        "            # update average test loss\n",
        "            test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss_value.data.item() - test_loss))\n",
        "\n",
        "            # convert logits to predicted class\n",
        "            # NOTE: the predicted class is the index of the max of the logits\n",
        "            pred = logits.data.max(1, keepdim=True)[1]  # =\n",
        "\n",
        "            # compare predictions to true label\n",
        "            correct += torch.sum(torch.squeeze(pred.eq(target.data.view_as(pred))).cpu())\n",
        "            total += data.size(0)\n",
        "\n",
        "            preds.extend(pred.data.cpu().numpy().squeeze())\n",
        "            actuals.extend(target.data.view_as(pred).cpu().numpy().squeeze())\n",
        "\n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
        "        100. * correct / total, correct, total))\n",
        "\n",
        "    return test_loss, preds, actuals\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(pred, truth, classes):\n",
        "\n",
        "    gt = pd.Series(truth, name='Ground Truth')\n",
        "    predicted = pd.Series(pred, name='Predicted')\n",
        "\n",
        "    confusion_matrix = pd.crosstab(gt, predicted)\n",
        "    confusion_matrix.index = classes\n",
        "    confusion_matrix.columns = classes\n",
        "\n",
        "    fig, sub = plt.subplots()\n",
        "    with sns.plotting_context(\"notebook\"):\n",
        "\n",
        "        ax = sns.heatmap(\n",
        "            confusion_matrix,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            ax=sub,\n",
        "            linewidths=0.5,\n",
        "            linecolor='lightgray',\n",
        "            cbar=False\n",
        "        )\n",
        "        ax.set_xlabel(\"truth\")\n",
        "        ax.set_ylabel(\"pred\")\n",
        "\n",
        "\n",
        "\n",
        "    return confusion_matrix"
      ],
      "metadata": {
        "id": "IjmKrGKFobnN",
        "outputId": "65ea5b83-6a14-4e43-c9cf-ced45a437782",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting livelossplot\n",
            "  Downloading livelossplot-0.5.5-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.7.1)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.1.1)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (3.1.2)\n",
            "Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.22.4)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (23.1)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.5.3)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.0.1)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.3.1)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2023.7.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.1.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.16.0)\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "luWm_N_bdymP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}