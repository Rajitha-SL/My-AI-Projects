{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajitha-SL/My-AI-Projects/blob/AI-and-ML-learning/cifar10_cnn_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPfZKLemmLf8"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "---\n",
        "In this notebook, we train a **CNN** to classify images from the CIFAR-10 database.\n",
        "\n",
        "The images in this database are small color images that fall into one of ten classes; some example images are pictured below.\n",
        "\n",
        "<img src='notebook_ims/cifar_data.png' width=70% height=70% />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0GSssdONm1Il",
        "outputId": "b5c81789-bf5b-4761-ce84-053882ab91c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7ec2a216-219d-467c-8d66-00ce26dd2326\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7ec2a216-219d-467c-8d66-00ce26dd2326\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving requirements.txt to requirements.txt\n",
            "Saving helpers.py to helpers.py\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'requirements.txt': b'opencv-python-headless==4.5.3.56\\nmatplotlib==3.4.3\\nnumpy==1.21.2\\npillow==7.0.0\\nbokeh==2.1.1\\ntorch==1.11.0\\ntorchvision==0.12.0\\ntqdm==4.63.0\\nipywidgets==7.6.5\\nlivelossplot==0.5.4\\npytest==7.1.1\\npandas==1.3.5\\nseaborn==0.11.2\\n',\n",
              " 'helpers.py': b'import matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\nimport torch\\nfrom livelossplot import PlotLosses\\nfrom livelossplot.outputs import MatplotlibPlot\\nfrom torch.utils.data.sampler import SubsetRandomSampler\\nfrom torchvision import datasets\\nfrom tqdm import tqdm\\n\\n\\ndef get_train_val_data_loaders(batch_size, valid_size, transforms, num_workers):\\n\\n    # Get the CIFAR10 training dataset from torchvision.datasets and set the transforms\\n    # We will split this further into train and validation in this function\\n    train_data = datasets.CIFAR10(\"data\", train=True, download=True, transform=transforms)\\n\\n    # Compute how many items we will reserve for the validation set\\n    n_tot = len(train_data)\\n    split = int(np.floor(valid_size * n_tot))\\n\\n    # compute the indices for the training set and for the validation set\\n    shuffled_indices = torch.randperm(n_tot)\\n    train_idx, valid_idx = shuffled_indices[split:], shuffled_indices[:split]\\n\\n    # define samplers for obtaining training and validation batches\\n    train_sampler = SubsetRandomSampler(train_idx)\\n    valid_sampler = SubsetRandomSampler(valid_idx)\\n\\n    # prepare data loaders (combine dataset and sampler)\\n    train_loader = torch.utils.data.DataLoader(\\n        train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers\\n    )\\n    valid_loader = torch.utils.data.DataLoader(\\n        train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers\\n    )\\n\\n    return train_loader, valid_loader\\n\\n\\ndef get_test_data_loader(batch_size, transforms, num_workers):\\n    # We use the entire test dataset in the test dataloader\\n    test_data = datasets.CIFAR10(\"data\", train=False, download=True, transform=transforms)\\n    test_loader = torch.utils.data.DataLoader(\\n        test_data, batch_size=batch_size, num_workers=num_workers\\n    )\\n\\n    return test_loader\\n\\n\\ndef train_one_epoch(train_dataloader, model, optimizer, loss):\\n    \"\"\"\\n    Performs one epoch of training\\n    \"\"\"\\n\\n    # Move model to GPU if available\\n    if torch.cuda.is_available():\\n        model.cuda()  # -\\n\\n    # Set the model in training mode\\n    # (so all layers that behave differently between training and evaluation,\\n    # like batchnorm and dropout, will select their training behavior)\\n    model.train()  # -\\n\\n    # Loop over the training data\\n    train_loss = 0.0\\n\\n    for batch_idx, (data, target) in tqdm(\\n        enumerate(train_dataloader),\\n        desc=\"Training\",\\n        total=len(train_dataloader),\\n        leave=True,\\n        ncols=80,\\n    ):\\n        # move data to GPU if available\\n        if torch.cuda.is_available():\\n            data, target = data.cuda(), target.cuda()\\n\\n        # 1. clear the gradients of all optimized variables\\n        optimizer.zero_grad()  # -\\n        # 2. forward pass: compute predicted outputs by passing inputs to the model\\n        output = model(data)  # =\\n        # 3. calculate the loss\\n        loss_value = loss(output, target)  # =\\n        # 4. backward pass: compute gradient of the loss with respect to model parameters\\n        loss_value.backward()  # -\\n        # 5. perform a single optimization step (parameter update)\\n        optimizer.step()  # -\\n\\n        # update average training loss\\n        train_loss = train_loss + (\\n            (1 / (batch_idx + 1)) * (loss_value.data.item() - train_loss)\\n        )\\n\\n    return train_loss\\n\\n\\ndef valid_one_epoch(valid_dataloader, model, loss):\\n    \"\"\"\\n    Validate at the end of one epoch\\n    \"\"\"\\n\\n    # During validation we don\\'t need to accumulate gradients\\n    with torch.no_grad():\\n\\n        # set the model to evaluation mode\\n        # (so all layers that behave differently between training and evaluation,\\n        # like batchnorm and dropout, will select their evaluation behavior)\\n        model.eval()  # -\\n\\n        # If the GPU is available, move the model to the GPU\\n        if torch.cuda.is_available():\\n            model.cuda()\\n\\n        # Loop over the validation dataset and accumulate the loss\\n        valid_loss = 0.0\\n        for batch_idx, (data, target) in tqdm(\\n            enumerate(valid_dataloader),\\n            desc=\"Validating\",\\n            total=len(valid_dataloader),\\n            leave=True,\\n            ncols=80,\\n        ):\\n            # move data to GPU if available\\n            if torch.cuda.is_available():\\n                data, target = data.cuda(), target.cuda()\\n\\n            # 1. forward pass: compute predicted outputs by passing inputs to the model\\n            output = model(data)  # =\\n            # 2. calculate the loss\\n            loss_value = loss(output, target)  # =\\n\\n            # Calculate average validation loss\\n            valid_loss = valid_loss + (\\n                (1 / (batch_idx + 1)) * (loss_value.data.item() - valid_loss)\\n            )\\n\\n    return valid_loss\\n\\n\\ndef optimize(data_loaders, model, optimizer, loss, n_epochs, save_path, interactive_tracking=False):\\n    # initialize tracker for minimum validation loss\\n    if interactive_tracking:\\n        liveloss = PlotLosses()\\n    else:\\n        liveloss = None\\n\\n    # Loop over the epochs and keep track of the minimum of the validation loss\\n    valid_loss_min = None\\n    logs = {}\\n\\n    for epoch in range(1, n_epochs + 1):\\n\\n        train_loss = train_one_epoch(\\n            data_loaders[\"train\"], model, optimizer, loss\\n        )\\n\\n        valid_loss = valid_one_epoch(data_loaders[\"valid\"], model, loss)\\n\\n        # print training/validation statistics\\n        print(\\n            \"Epoch: {} \\\\tTraining Loss: {:.6f} \\\\tValidation Loss: {:.6f}\".format(\\n                epoch, train_loss, valid_loss\\n            )\\n        )\\n\\n        # If the validation loss decreases by more than 1%, save the model\\n        if valid_loss_min is None or (\\n                (valid_loss_min - valid_loss) / valid_loss_min > 0.01\\n        ):\\n            print(f\"New minimum validation loss: {valid_loss:.6f}. Saving model ...\")\\n\\n            # Save the weights to save_path\\n            torch.save(model.state_dict(), save_path)  # -\\n\\n            valid_loss_min = valid_loss\\n\\n        # Log the losses and the current learning rate\\n        if interactive_tracking:\\n            logs[\"loss\"] = train_loss\\n            logs[\"val_loss\"] = valid_loss\\n\\n            liveloss.update(logs)\\n            liveloss.send()\\n\\n            \\ndef one_epoch_test(test_dataloader, model, loss):\\n    # monitor test loss and accuracy\\n    test_loss = 0.\\n    correct = 0.\\n    total = 0.\\n\\n    # we do not need the gradients\\n    with torch.no_grad():\\n\\n        # set the model to evaluation mode\\n        model.eval()  # -\\n\\n        # if the GPU is available, move the model to the GPU\\n        if torch.cuda.is_available():\\n            model = model.cuda()\\n\\n        # Loop over test dataset\\n        # We also accumulate predictions and targets so we can return them\\n        preds = []\\n        actuals = []\\n        \\n        for batch_idx, (data, target) in tqdm(\\n                enumerate(test_dataloader),\\n                desc=\\'Testing\\',\\n                total=len(test_dataloader),\\n                leave=True,\\n                ncols=80\\n        ):\\n            # move data to GPU if available\\n            if torch.cuda.is_available():\\n                data, target = data.cuda(), target.cuda()\\n\\n            # 1. forward pass: compute predicted outputs by passing inputs to the model\\n            logits = model(data)  # =\\n            # 2. calculate the loss\\n            loss_value = loss(logits, target).detach()  # =\\n\\n            # update average test loss\\n            test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss_value.data.item() - test_loss))\\n\\n            # convert logits to predicted class\\n            # NOTE: the predicted class is the index of the max of the logits\\n            pred = logits.data.max(1, keepdim=True)[1]  # =\\n\\n            # compare predictions to true label\\n            correct += torch.sum(torch.squeeze(pred.eq(target.data.view_as(pred))).cpu())\\n            total += data.size(0)\\n            \\n            preds.extend(pred.data.cpu().numpy().squeeze())\\n            actuals.extend(target.data.view_as(pred).cpu().numpy().squeeze())\\n\\n    print(\\'Test Loss: {:.6f}\\\\n\\'.format(test_loss))\\n\\n    print(\\'\\\\nTest Accuracy: %2d%% (%2d/%2d)\\' % (\\n        100. * correct / total, correct, total))\\n\\n    return test_loss, preds, actuals\\n\\n\\ndef plot_confusion_matrix(pred, truth, classes):\\n\\n    gt = pd.Series(truth, name=\\'Ground Truth\\')\\n    predicted = pd.Series(pred, name=\\'Predicted\\')\\n\\n    confusion_matrix = pd.crosstab(gt, predicted)\\n    confusion_matrix.index = classes\\n    confusion_matrix.columns = classes\\n    \\n    fig, sub = plt.subplots()\\n    with sns.plotting_context(\"notebook\"):\\n\\n        ax = sns.heatmap(\\n            confusion_matrix, \\n            annot=True, \\n            fmt=\\'d\\',\\n            ax=sub, \\n            linewidths=0.5, \\n            linecolor=\\'lightgray\\', \\n            cbar=False\\n        )\\n        ax.set_xlabel(\"truth\")\\n        ax.set_ylabel(\"pred\")\\n\\n    \\n\\n    return confusion_matrix\\n'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EExD3viCmLf_",
        "outputId": "b1c58264-ef63-4716-a81c-fe0544d322a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8SPWeiymLgB"
      },
      "source": [
        "### Test for [CUDA](http://pytorch.org/docs/stable/cuda.html)\n",
        "\n",
        "Since these are larger (32x32x3) images, it may prove useful to speed up your training time by using a GPU. CUDA is a parallel computing platform and CUDA Tensors are the same as typical Tensors, only they utilize GPU's for computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cg9k8MmBmLgC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH66nsbtmLgD"
      },
      "source": [
        "---\n",
        "## Load the [Data](http://pytorch.org/docs/stable/torchvision/datasets.html)\n",
        "\n",
        "Downloading may take a minute. We load in the training and test data, split the training data into a training and validation set, then create DataLoaders for each of these sets of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uLRIFRBmLgD"
      },
      "outputs": [],
      "source": [
        "!pip install livelossplot\n",
        "import torchvision.transforms\n",
        "import multiprocessing\n",
        "from helpers import get_train_val_data_loaders, get_test_data_loader\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# convert data to a normalized torch.FloatTensor\n",
        "transforms = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "# If you want to refresh how to load and split data in pytorch, open the helpers.py file\n",
        "# and read the code. We have documented it with comments so you can follow along easily\n",
        "train_dl, valid_dl = get_train_val_data_loaders(batch_size, valid_size, transforms, num_workers)\n",
        "test_dl = get_test_data_loader(batch_size, transforms, num_workers)\n",
        "\n",
        "# For convenience let's group them together in a dictionary\n",
        "data_loaders = {\n",
        "    'train': train_dl,\n",
        "    'valid': valid_dl,\n",
        "    'test': test_dl\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbGpQV6CmLgF"
      },
      "outputs": [],
      "source": [
        "# specify the image classes\n",
        "classes = [\n",
        "    \"airplane\",\n",
        "    \"automobile\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"deer\",\n",
        "    \"dog\",\n",
        "    \"frog\",\n",
        "    \"horse\",\n",
        "    \"ship\",\n",
        "    \"truck\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpN83PPSmLgG"
      },
      "source": [
        "### Visualize a Batch of Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYC1qPc2mLgG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img, sub):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    sub.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "    sub.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRBhjTSImLgG"
      },
      "outputs": [],
      "source": [
        "# obtain one batch of training images\n",
        "dataiter = iter(data_loaders['train'])\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "# display 20 images\n",
        "# NOTE: make sure your batch size is at least 20\n",
        "fig, subs = plt.subplots(2, 10, figsize=(25, 4))\n",
        "for i, sub in enumerate(subs.flatten()):\n",
        "    imshow(images[i], sub)\n",
        "    sub.set_title(classes[labels[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xluH8sTmmLgH"
      },
      "source": [
        "### View an Image in More Detail\n",
        "\n",
        "Here, we look at the normalized red, green, and blue (RGB) color channels as three separate, grayscale intensity images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umt0MgfbmLgH"
      },
      "outputs": [],
      "source": [
        "rgb_img = np.squeeze(images[3])\n",
        "channels = ['composite', 'red', 'green', 'blue']\n",
        "cmaps = [None, 'Reds', 'Greens', 'Blues']\n",
        "\n",
        "fig, subs = plt.subplots(1, 4)\n",
        "\n",
        "for i, sub in enumerate(subs.flatten()):\n",
        "\n",
        "    if i == 0:\n",
        "        imshow(rgb_img, sub)\n",
        "    else:\n",
        "        img = rgb_img[i-1]\n",
        "        sub.imshow(img, cmap=cmaps[i])\n",
        "        sub.set_title(channels[i])\n",
        "        sub.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ2Un-8GmLgH"
      },
      "source": [
        "---\n",
        "## Define the Network [Architecture](http://pytorch.org/docs/stable/nn.html)\n",
        "\n",
        "This time, you'll define a CNN architecture. Instead of an MLP, which used linear, fully-connected layers, you'll use the following:\n",
        "* [Convolutional layers](https://pytorch.org/docs/stable/nn.html#conv2d), which can be thought of as stack of filtered images.\n",
        "* [Maxpooling layers](https://pytorch.org/docs/stable/nn.html#maxpool2d), which reduce the x-y size of an input, keeping only the most _active_ pixels from the previous layer.\n",
        "* The usual Linear + Dropout layers to avoid overfitting and produce a 10-dim output.\n",
        "\n",
        "A network with 2 convolutional layers is shown in the image below and in the code, and you've been given starter code with one convolutional and one maxpooling layer.\n",
        "\n",
        "<img src='notebook_ims/2_layer_conv.png' height=50% width=50% />\n",
        "\n",
        "#### TODO: Define a model with multiple convolutional layers, and define the feedforward metwork behavior.\n",
        "\n",
        "The more convolutional layers you include, the more complex patterns in color and shape a model can detect. It's suggested that your final model include 2 or 3 convolutional layers as well as linear layers + dropout in between to avoid overfitting.\n",
        "\n",
        "It's good practice to look at existing research and implementations of related models as a starting point for defining your own models. You may find it useful to look at [this PyTorch classification example](https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py) to help decide on a final structure.\n",
        "\n",
        "#### Output volume for a convolutional layer\n",
        "\n",
        "To compute the output size of a given convolutional layer we can perform the following calculation (taken from [Stanford's cs231n course](http://cs231n.github.io/convolutional-networks/#layers)):\n",
        "> We can compute the spatial size of the output volume as a function of the input volume size (W), the kernel/filter size (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. The correct formula for calculating how many neurons define the output_W is given by `(W−F+2P)/S+1`.\n",
        "\n",
        "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFQqCyqZmLgI"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# define the CNN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_classes=10):\n",
        "\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# create a complete CNN\n",
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if train_on_gpu:\n",
        "    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAsSkvhMmLgI"
      },
      "source": [
        "## How many parameters?\n",
        "\n",
        "Now that you built the network, can you compute with pen and paper how many parameters does your network have? Does your answer match the output of the following cell?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYaERUQ4mLgI"
      },
      "outputs": [],
      "source": [
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4szkSwm2mLgI"
      },
      "source": [
        "## Visualize your network (optional)\n",
        "\n",
        "You can visualize your achitecture by using netron.app. Just execute the following cell (which will save the network to a file called \"cifar10_network.pt\" in this directory), then download the produced `cifar10_network.pt` to your computer. Finally, go to [Netron.app](https://netron.app) and click on `Open Model`, and select the file you just downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNcIOztpmLgI"
      },
      "outputs": [],
      "source": [
        "scripted = torch.jit.script(model)\n",
        "torch.jit.save(scripted, \"cifar10_network.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJQkJWhNmLgJ"
      },
      "source": [
        "### Specify [Loss Function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [Optimizer](http://pytorch.org/docs/stable/optim.html)\n",
        "\n",
        "Decide on a loss and optimization function that is best suited for this classification task. The linked code examples from above, may be a good starting point; [this PyTorch classification example](https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py). Pay close attention to the value for **learning rate** as this value determines how your model converges to a small error.\n",
        "\n",
        "#### TODO: Define the loss and optimizer and see how these choices change the loss over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "985xoElgmLgJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "4f67c45d-193e-4b11-840d-08cd12854950"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-bbbe03047be3>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    loss = # YOUR CODE HERE\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# specify loss function (categorical cross-entropy)\n",
        "loss = # YOUR CODE HERE\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLXm-CKVmLgJ"
      },
      "source": [
        "---\n",
        "## Train the Network\n",
        "\n",
        "Remember to look at how the training and validation loss decreases over time; if the validation loss ever increases it indicates possible overfitting. (In fact, in the below example, we could have stopped around epoch 33 or so!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHqEExgwmLgK"
      },
      "outputs": [],
      "source": [
        "from helpers import optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AUX-QeHmLgK"
      },
      "outputs": [],
      "source": [
        "optimize(\n",
        "    data_loaders,\n",
        "    model,\n",
        "    optimizer,\n",
        "    loss,\n",
        "    20,\n",
        "    \"cifar10_best_valid.pt\",\n",
        "    interactive_tracking=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK5olgTWmLgK"
      },
      "source": [
        "###  Load the Model with the Lowest Validation Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e505FrzlmLgK"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('cifar10_best_valid.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_vVu2CFmLgK"
      },
      "source": [
        "---\n",
        "## Test the Trained Network\n",
        "\n",
        "Test your trained model on previously unseen data! A \"good\" result will be a CNN that gets around 70% (or more, try your best!) accuracy on these test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPMSBT-1mLgK"
      },
      "outputs": [],
      "source": [
        "from helpers import one_epoch_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0NcMb_1mLgL"
      },
      "outputs": [],
      "source": [
        "test_loss, preds, actuals = one_epoch_test(data_loaders['valid'], model, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM0U0D06mLgL"
      },
      "outputs": [],
      "source": [
        "from helpers import plot_confusion_matrix\n",
        "\n",
        "cm = plot_confusion_matrix(preds, actuals, classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEfnEjEtmLgL"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy by class:\\n\")\n",
        "for i, col in enumerate(cm):\n",
        "    print(f\"    {col:11s}: {cm[col][i] / cm[col].sum():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTD8PMzvmLgL"
      },
      "source": [
        "### Question: What are your model's weaknesses and how might they be improved?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hviGdD9TmLgL"
      },
      "source": [
        "**Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh_Cnav3mLgM"
      },
      "source": [
        "_NOTE_: we will see how to improve the performances even further with simple things like data augmentation and batch norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doVvB5GjmLgM"
      },
      "source": [
        "### Visualize Sample Test Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLKt-hOwmLgM"
      },
      "outputs": [],
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(data_loaders['test'])\n",
        "\n",
        "for i in range(2):\n",
        "    images, labels = dataiter.next()\n",
        "    images.numpy()\n",
        "\n",
        "    # move model inputs to cuda, if GPU available\n",
        "    if train_on_gpu:\n",
        "        images = images.cuda()\n",
        "\n",
        "    # get sample outputs\n",
        "    output = model(images)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, preds_tensor = torch.max(output, 1)\n",
        "    preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "    # plot the images in the batch, along with predicted and true labels\n",
        "    fig, subs = plt.subplots(2, 10, figsize=(25, 4))\n",
        "    for i, ax in enumerate(subs.flatten()):\n",
        "        imshow(images[i].cpu().numpy(), ax)\n",
        "        ax.set_title(\"{} ({})\".format(classes[preds[i]], classes[labels[i]]),\n",
        "                     color=(\"green\" if preds[i]==labels[i].item() else \"red\"))\n",
        "        ax.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oReTlr00mLgM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}